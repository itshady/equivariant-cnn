{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# E(n)-Equivariant Steerable CNNs  -  A concrete example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "\n",
    "from escnn import gspaces\n",
    "from escnn import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build a **Steerable CNN** and try it on MNIST.\n",
    "\n",
    "Let's also use a group a bit larger: we now build a model equivariant to $8$ rotations.\n",
    "We indicate the group of $N$ discrete rotations as $C_N$, i.e. the **cyclic group** of order $N$.\n",
    "In this case, we will use $C_8$.\n",
    "\n",
    "Because the inputs are still gray-scale images, the input type of the model is again a *scalar field*.\n",
    "\n",
    "However, internally we use *regular fields*: this is equivalent to a *group-equivariant convolutional neural network*.\n",
    "\n",
    "Finally, we build *invariant* features for the final classification task by pooling over the group using *Group Pooling*.\n",
    "\n",
    "The final classification is performed by a two fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "Here is the definition of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C8SteerableCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes=10):\n",
    "        \n",
    "        super(C8SteerableCNN, self).__init__()\n",
    "        \n",
    "        # the model is equivariant under rotations by 45 degrees, modelled by C8\n",
    "        self.r2_act = gspaces.rot2dOnR2(N=8)\n",
    "        \n",
    "        # the input image is a scalar field, corresponding to the trivial representation\n",
    "        in_type = nn.FieldType(self.r2_act, [self.r2_act.trivial_repr])\n",
    "        \n",
    "        # we store the input type for wrapping the images into a geometric tensor during the forward pass\n",
    "        self.input_type = in_type\n",
    "        \n",
    "        # convolution 1\n",
    "        # first specify the output type of the convolutional layer\n",
    "        # we choose 24 feature fields, each transforming under the regular representation of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 24*[self.r2_act.regular_repr])\n",
    "        self.block1 = nn.SequentialModule(\n",
    "            nn.MaskModule(in_type, 29, margin=1), # rotating a square image causes corners and edges to move into previously empty regions. This makes my image inherently not symmetric. Hence masking a circle around it to bring the symmetry back\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=7, padding=1, bias=False),\n",
    "            nn.InnerBatchNorm(out_type), # equivariant version of Batch Normalization. It normalizes all channels together\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution 2\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block1.out_type\n",
    "        # the output type of the second convolution layer are 48 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n",
    "        self.block2 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool1 = nn.SequentialModule(\n",
    "            nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        ) # pooling is invariant\n",
    "        \n",
    "        # convolution 3\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block2.out_type\n",
    "        # the output type of the third convolution layer are 48 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n",
    "        self.block3 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution 4\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block3.out_type\n",
    "        # the output type of the fourth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n",
    "        self.block4 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool2 = nn.SequentialModule(\n",
    "            nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        )\n",
    "        \n",
    "        # convolution 5\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block4.out_type\n",
    "        # the output type of the fifth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n",
    "        self.block5 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution 6\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block5.out_type\n",
    "        # the output type of the sixth convolution layer are 64 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 64*[self.r2_act.regular_repr])\n",
    "        self.block6 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool3 = nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=1, padding=0)\n",
    "        \n",
    "        self.gpool = nn.GroupPooling(out_type)\n",
    "        \n",
    "        # number of output channels\n",
    "        c = self.gpool.out_type.size\n",
    "        \n",
    "        # Fully Connected\n",
    "        self.fully_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(c, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.ELU(inplace=True),\n",
    "            torch.nn.Linear(64, n_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # wrap the input tensor in a GeometricTensor\n",
    "        # (associate it with the input type)\n",
    "        x = nn.GeometricTensor(input, self.input_type)\n",
    "        \n",
    "        # apply each equivariant block\n",
    "        \n",
    "        # Each layer has an input and an output type\n",
    "        # A layer takes a GeometricTensor in input.\n",
    "        # This tensor needs to be associated with the same representation of the layer's input type\n",
    "        #\n",
    "        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n",
    "        # As a result, consecutive layers need to have matching input/output types\n",
    "        print(x.shape)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        \n",
    "        # pool over the spatial dimensions\n",
    "        print(x.shape)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # pool over the group\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.gpool(x)\n",
    "        print(x.shape)\n",
    "        a=1/0\n",
    "\n",
    "\n",
    "        # unwrap the output GeometricTensor\n",
    "        # (take the Pytorch tensor and discard the associated representation)\n",
    "        x = x.tensor\n",
    "        \n",
    "        # classify with the final fully connected layers)\n",
    "        x = self.fully_net(x.reshape(x.shape[0], -1))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(         \n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=3, # prob drop this to 3 (which is normally done)\n",
    "                stride=1,                   \n",
    "                padding=1, # not much of a benefit + computationally efficient w/out, but if it was 0, the dimensions would reduce to 26x26\n",
    "            ),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(         \n",
    "            torch.nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            torch.nn.ReLU(),                      \n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),                \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(32 * 7 * 7, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 10) # reduce number of nodes\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.fc(x)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the model on *rotated* MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "unzip:  cannot find or open mnist_rotation_new.zip, mnist_rotation_new.zip.zip or mnist_rotation_new.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# download the dataset\n",
    "!wget -nc http://www.iro.umontreal.ca/~lisa/icml2007data/mnist_rotation_new.zip\n",
    "# uncompress the zip file\n",
    "!unzip -n mnist_rotation_new.zip -d mnist_rotation_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import RandomRotation\n",
    "from torchvision.transforms import Pad\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Device configuration\n",
    "comp = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    comp = 'cuda' # nvidia gpu parallelization\n",
    "elif torch.backends.mps.is_available():\n",
    "    comp = 'mps' # mac Metal Performance Shaders (high performance gpu)\n",
    "\n",
    "device = torch.device(comp)\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistRotDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mode, transform=None):\n",
    "        assert mode in ['train', 'test']\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_train_valid.amat\"\n",
    "        else:\n",
    "            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_test.amat\"\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        data = np.loadtxt(file, delimiter=' ')\n",
    "            \n",
    "        self.images = data[:, :-1].reshape(-1, 28, 28).astype(np.float32)\n",
    "        self.labels = data[:, -1].astype(np.int64)\n",
    "        self.num_samples = len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.images[index], self.labels[index]\n",
    "        image = Image.fromarray(image, mode='F')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Padding: images are 28x28; we pad the right and bottom with 1 pixel; images are padded to have shape 29x29.\n",
    "# this allows to use odd-size filters with stride 2 when downsampling a feature map in the model\n",
    "pad = Pad((0, 0, 1, 1), fill=0)\n",
    "\n",
    "# to reduce interpolation artifacts (e.g. when testing the model on rotated images),\n",
    "# we upsample an image by a factor of 3, rotate it and finally downsample it again (resize1)\n",
    "# 1. Resize from 28x28 to 87x87 (3x upsampling)\n",
    "resize1 = Resize(87)\n",
    "# 2. Resize back down to 29x29\n",
    "resize2 = Resize(29)\n",
    "\n",
    "# applies random rotation\n",
    "# 180.  ->  Random rotation from -180° to +180°\n",
    "# interpolation=InterpolationMode.BILINEAR  ->  Smooths pixels during rotation, reducing aliasing artifacts\n",
    "# expand=False  ->   Keeps the output image the same size (may crop corners)\n",
    "randomRotation = RandomRotation(180., interpolation=InterpolationMode.BILINEAR, expand=False)\n",
    "\n",
    "totensor = ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = C8SteerableCNN().to(device)\n",
    "# model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now randomly initialized. \n",
    "Therefore, we do not expect it to produce the right class probabilities.\n",
    "\n",
    "However, the model should still produce the same output for rotated versions of the same image.\n",
    "This is true for rotations by multiples of $\\frac{\\pi}{2}$, but is only approximate for rotations by $\\frac{\\pi}{4}$.\n",
    "\n",
    "Let's test it on a random test image:\n",
    "we feed eight rotated versions of the first image in the test set and print the output logits of the model for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(model: torch.nn.Module, x: Image):\n",
    "    np.set_printoptions(linewidth=10000)\n",
    "    \n",
    "    # evaluate the `model` on 8 rotated versions of the input image `x`\n",
    "    model.eval()\n",
    "    \n",
    "    x = resize1(pad(x))\n",
    "    \n",
    "    print()\n",
    "    print('##########################################################################################')\n",
    "    header = 'angle |  ' + '  '.join([\"{:6d}\".format(d) for d in range(10)])\n",
    "    print(header)\n",
    "    with torch.no_grad():\n",
    "        for r in range(8):\n",
    "            x_transformed = totensor(resize2(x.rotate(r*45., Image.BILINEAR))).reshape(1, 1, 29, 29)\n",
    "            x_transformed = x_transformed.to(device)\n",
    "\n",
    "            y = model(x_transformed)\n",
    "            y = y.to('cpu').numpy().squeeze()\n",
    "            \n",
    "            angle = r * 45\n",
    "            print(\"{:5d} : {}\".format(angle, y))\n",
    "    print('##########################################################################################')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the test set    \n",
    "raw_mnist_test = MnistRotDataset(mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########################################################################################\n",
      "angle |       0       1       2       3       4       5       6       7       8       9\n",
      "torch.Size([1, 1, 29, 29])\n",
      "torch.Size([1, 512, 5, 5])\n",
      "torch.Size([1, 512, 1, 1])\n",
      "torch.Size([1, 64, 1, 1])\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(raw_mnist_test))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# evaluate the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 18\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m x_transformed \u001b[38;5;241m=\u001b[39m totensor(resize2(x\u001b[38;5;241m.\u001b[39mrotate(r\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m45.\u001b[39m, Image\u001b[38;5;241m.\u001b[39mBILINEAR)))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m29\u001b[39m, \u001b[38;5;241m29\u001b[39m)\n\u001b[1;32m     16\u001b[0m x_transformed \u001b[38;5;241m=\u001b[39m x_transformed\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 18\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_transformed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     21\u001b[0m angle \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m45\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[58], line 136\u001b[0m, in \u001b[0;36mC8SteerableCNN.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpool(x)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 136\u001b[0m a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# unwrap the output GeometricTensor\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# (take the Pytorch tensor and discard the associated representation)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtensor\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# retrieve the first image from the test set\n",
    "x, y = next(iter(raw_mnist_test))\n",
    "\n",
    "# evaluate the model\n",
    "test_model(model, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model is already almost invariant.\n",
    "However, we still observe small fluctuations in the outputs.\n",
    "\n",
    "This is because the model contains some operations which might break equivariance.\n",
    "For instance, every convolution includes a padding of $2$ pixels per side. This is adds information about the actual orientation of the grid where the image/feature map is sampled because the padding is not rotated with the image. \n",
    "\n",
    "During training, the model will observe rotated patterns and will learn to ignore the noise coming from the padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's train the model now.\n",
    "The model is exactly the same used to train a normal *PyTorch* architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "    pad,\n",
    "    resize1,\n",
    "    randomRotation,\n",
    "    resize2,\n",
    "    totensor,\n",
    "])\n",
    "\n",
    "mnist_train = MnistRotDataset(mode='train', transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=64)\n",
    "\n",
    "\n",
    "test_transform = Compose([\n",
    "    pad,\n",
    "    totensor,\n",
    "])\n",
    "mnist_test = MnistRotDataset(mode='test', transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=64)\n",
    "\n",
    "loader = {\n",
    "    \"train\": train_loader,\n",
    "    \"test\": test_loader\n",
    "}\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct_test \u001b[38;5;241m/\u001b[39m total_test\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_test_loss, test_accuracy\n\u001b[0;32m---> 33\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[42], line 18\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(cnn, test_loader)\u001b[0m\n\u001b[1;32m     15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m cnn(images)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Apply softmax to get probabilities\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[1;32m     21\u001b[0m total_test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/functional.py:2140\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2138\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2140\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# Hady\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to test the model using test data\n",
    "def evaluate(cnn, test_loader):\n",
    "    cnn.eval()  # Set model to evaluation mode\n",
    "    total_test_loss = 0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, labels in test_loader[\"test\"]:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = cnn(images)[0]\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "            loss = loss_function(outputs, labels)\n",
    "            total_test_loss += loss.item()\n",
    "            \n",
    "            # Predictions for test accuracy\n",
    "            _, predicted = torch.max(probabilities, 1)\n",
    "            print(probabilities)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    return avg_test_loss, test_accuracy\n",
    "\n",
    "test_loss, test_accuracy = evaluate(model, loader)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "#1:13 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# NOTE: If loss is bouncing around a lot, that means the lr is too high\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# NOTE: train doesnt always show improvements that test does. Hence running a test every epoch and comparing the \u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Underfitting: What it looks like\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# both train and test accuracy plateau early\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 30\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(num_epochs, cnn, loaders)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Apply softmax to get probabilities\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, labels)\n\u001b[1;32m     33\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/functional.py:2140\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2138\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2140\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# Hady added\n",
    "import torch.nn.functional as F\n",
    "import matplotlib as plt\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "def train_and_evaluate(num_epochs, cnn, loaders):\n",
    "    # Lists to store loss and accuracy for each epoch\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        cnn.train()  # Set model to training mode\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for images, labels in loaders['train']:  \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            output = cnn(images)[0]\n",
    "            print(output.shape)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = F.softmax(output, dim=1)\n",
    "\n",
    "            loss = loss_function(output, labels)\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Predictions for training accuracy\n",
    "            _, predicted = torch.max(probabilities, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        avg_train_loss = total_train_loss / len(loaders['train'])\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        test_loss, test_accuracy = evaluate(cnn, loaders['test'])\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "    # Plot training and test accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(range(1, num_epochs + 1), test_accuracies, label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training vs. Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and test loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs. Test Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "train_and_evaluate(num_epochs, model, loader)\n",
    "\n",
    "\n",
    "# NOTE: If loss is bouncing around a lot, that means the lr is too high\n",
    "# NOTE: train doesnt always show improvements that test does. Hence running a test every epoch and comparing the \n",
    "\n",
    "# Overfitting: What it looks like\n",
    "# train accuracy will increase, but test accuracy decreases\n",
    "\n",
    "# Underfitting: What it looks like\n",
    "# both train and test accuracy plateau early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "188\n",
      "inner 0\n",
      "inner 1\n",
      "inner 2\n",
      "inner 3\n",
      "inner 4\n",
      "inner 5\n",
      "inner 6\n",
      "inner 7\n",
      "inner 8\n",
      "inner 9\n",
      "inner 10\n",
      "inner 11\n",
      "inner 12\n",
      "inner 13\n",
      "inner 14\n",
      "inner 15\n",
      "inner 16\n",
      "inner 17\n",
      "inner 18\n",
      "inner 19\n",
      "inner 20\n",
      "inner 21\n",
      "inner 22\n",
      "inner 23\n",
      "inner 24\n",
      "inner 25\n",
      "inner 26\n",
      "inner 27\n",
      "inner 28\n",
      "inner 29\n",
      "inner 30\n",
      "inner 31\n",
      "inner 32\n",
      "inner 33\n",
      "inner 34\n",
      "inner 35\n",
      "inner 36\n",
      "inner 37\n",
      "inner 38\n",
      "inner 39\n",
      "inner 40\n",
      "inner 41\n",
      "inner 42\n",
      "inner 43\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(y, t)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[37], line 123\u001b[0m, in \u001b[0;36mC8SteerableCNN.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    120\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock4(x)\n\u001b[1;32m    121\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(x)\n\u001b[0;32m--> 123\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock6(x)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# pool over the spatial dimensions\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/escnn/nn/modules/sequential_module.py:88\u001b[0m, in \u001b[0;36mSequentialModule.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     86\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 88\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_type\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/escnn/nn/modules/batchnormalization/inner.py:158\u001b[0m, in \u001b[0;36mInnerBatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    153\u001b[0m batchnorm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_norm_[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m contiguous:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# if the fields were contiguous, we can use slicing\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     output[:, indices[\u001b[38;5;241m0\u001b[39m]:indices[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m batchnorm(\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     )\u001b[38;5;241m.\u001b[39mview(b, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mshape)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# otherwise we have to use indexing\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     output[:, indices, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m batchnorm(\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mtensor[:, indices]\u001b[38;5;241m.\u001b[39mview(b, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, s, \u001b[38;5;241m*\u001b[39mshape)\n\u001b[1;32m    164\u001b[0m     )\u001b[38;5;241m.\u001b[39mview(b, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_batches = 25\n",
    "for epoch in range(11):\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    print(len(train_loader))\n",
    "    for i, (x, t) in enumerate(train_loader):\n",
    "        # if i >= max_batches:\n",
    "        #     break\n",
    "        print(\"inner\", i)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = model(x)\n",
    "\n",
    "        loss = loss_function(y, t)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            print(len(test_loader))\n",
    "            for i, (x, t) in enumerate(test_loader):\n",
    "                # if i >= max_batches:\n",
    "                #     break\n",
    "                print(\"inner test\", i)\n",
    "\n",
    "                x = x.to(device)\n",
    "                t = t.to(device)\n",
    "                \n",
    "                y = model(x)\n",
    "\n",
    "                _, prediction = torch.max(y.data, 1)\n",
    "                total += t.shape[0]\n",
    "                correct += (prediction == t).sum().item()\n",
    "        print(f\"epoch {epoch} | test accuracy: {correct/total*100.}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########################################################################################\n",
      "angle |       0       1       2       3       4       5       6       7       8       9\n",
      "    0 : [-0.0811 -0.5303 -2.0083 -1.6987 -0.951  -3.4474  8.7345 -1.9093 -2.324  -0.8096]\n",
      "   45 : [-0.0796 -0.3873 -1.6391 -1.8317 -0.5286 -3.4706  8.6717 -2.3546 -2.0176 -0.8092]\n",
      "   90 : [-0.0811 -0.5303 -2.0083 -1.6987 -0.951  -3.4474  8.7345 -1.9093 -2.324  -0.8096]\n",
      "  135 : [-0.0796 -0.3873 -1.6391 -1.8317 -0.5286 -3.4706  8.6717 -2.3546 -2.0176 -0.8092]\n",
      "  180 : [-0.0811 -0.5303 -2.0083 -1.6987 -0.951  -3.4474  8.7345 -1.9093 -2.324  -0.8096]\n",
      "  225 : [-0.0796 -0.3873 -1.6391 -1.8317 -0.5286 -3.4706  8.6717 -2.3546 -2.0176 -0.8092]\n",
      "  270 : [-0.0811 -0.5303 -2.0083 -1.6987 -0.951  -3.4474  8.7345 -1.9093 -2.324  -0.8096]\n",
      "  315 : [-0.0796 -0.3873 -1.6391 -1.8317 -0.5286 -3.4706  8.6717 -2.3546 -2.0176 -0.8092]\n",
      "##########################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# retrieve the first image from the test set\n",
    "x, y = next(iter(raw_mnist_test))\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "test_model(model, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
