{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultrasound Microrobot Preprocessing and Dataset\n",
    "\n",
    "This notebook demonstrates a modular approach to:\n",
    "\n",
    "1. **Preprocess the Ultrasound Data**: Convert ultrasound images (1920×1080) and corresponding bounding-box label files into a single `.amat` file per microrobot type.\n",
    "2. **Create a PyTorch Dataset**: Define a dataset class (`USMicroMagDataset`) that loads the `.amat` file, reshapes the image data, and makes it available to your model.\n",
    "\n",
    "The folder structure is assumed to be as follows:\n",
    "\n",
    "```\n",
    "UsMicroMagSet-main/\n",
    "├── sample.yaml\n",
    "├── images/\n",
    "│   ├── train/   (contains many .png images)\n",
    "│   ├── val/     (contains many .png images)\n",
    "│   └── test/    (contains many .png images)\n",
    "└── labels/\n",
    "    ├── train/   (each image has a corresponding .txt file with bounding box info)\n",
    "    ├── val/\n",
    "    └── test/\n",
    "```\n",
    "\n",
    "The `sample.yaml` might look like:\n",
    "\n",
    "```yaml\n",
    "path: ../dataset/cylinder\n",
    "train:\n",
    "  - images/train\n",
    "val:\n",
    "  - images/val\n",
    "test:\n",
    "  - images/test\n",
    "nc: 1 \n",
    "names: [\"cylinder\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# E(n)-Equivariant Steerable CNNs  -  A concrete example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "\n",
    "from escnn import gspaces\n",
    "from escnn import nn\n",
    "\n",
    "# For preprocessing data\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# for Part 3\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import RandomRotation\n",
    "from torchvision.transforms import Pad\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import InterpolationMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Preprocess Ultrasound Data\n",
    "\n",
    "This function reads ultrasound images and corresponding label files, then writes a space-delimited `.amat` file. Each row in the `.amat` file contains the flattened pixel values (from a 1920×1080 grayscale image) followed by the label values (for example, 5 numbers representing a bounding box)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/hibrahim/Documents/Class/Machine Learning/research_coding/code\n",
      "Saved USMicroMagSet_processed/ultrasound_flagella_train_128.amat with shape (128, 2073604)\n",
      "Saved USMicroMagSet_processed/ultrasound_flagella_test_128.amat with shape (128, 2073604)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_ultrasound_data(microrobot_folder, max_images=None):\n",
    "    \"\"\"\n",
    "    Preprocess ultrasound images and labels for a given microrobot type.\n",
    "\n",
    "    Args:\n",
    "        microrobot_folder (str): Path to the folder containing sample.yaml, images/, and labels/\n",
    "        max_images (int or None): Maximum number of images to include per split (train/test).\n",
    "                                  If None, includes all available images.\n",
    "    \n",
    "    Expects the following folder structure inside `microrobot_folder`:\n",
    "    \n",
    "        sample.yaml\n",
    "        images/\n",
    "            train/   -- training images (.png)\n",
    "            test/    -- testing images (.png)\n",
    "            val/     -- validation images (.png)\n",
    "        labels/\n",
    "            train/   -- training label files (.txt)\n",
    "            test/    -- testing label files (.txt)\n",
    "            val/     -- validation label files (.txt)\n",
    "    \n",
    "    The sample.yaml file is assumed to contain, for example:\n",
    "    \n",
    "        path: ../dataset/cylinder\n",
    "        train:\n",
    "          - images/train\n",
    "        val:\n",
    "          - images/val\n",
    "        test:\n",
    "          - images/test\n",
    "        nc: 1 \n",
    "        names: [\"cylinder\"]\n",
    "    \n",
    "    For training, we combine images from both \"train\" and \"val\" splits.\n",
    "    Each output .amat file will have one row per image:\n",
    "       [ flattened_pixels (1920x1080)  label_values ]\n",
    "    \"\"\"\n",
    "    # Path to the sample.yaml file\n",
    "    print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "    sample_yaml_path = os.path.join(microrobot_folder, \"sample.yaml\")\n",
    "    with open(sample_yaml_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Define splits: combine \"train\" and \"val\" for training; test remains separate\n",
    "    splits = {\"train\": [], \"test\": []}\n",
    "    \n",
    "    # Combine train and validation splits\n",
    "    for key in [\"train\", \"val\"]:\n",
    "        if key in config and config[key]:\n",
    "            for rel_dir in config[key]:\n",
    "                splits[\"train\"].append(os.path.join(microrobot_folder, rel_dir))\n",
    "    \n",
    "    # Test split\n",
    "    if \"test\" in config and config[\"test\"]:\n",
    "        for rel_dir in config[\"test\"]:\n",
    "            splits[\"test\"].append(os.path.join(microrobot_folder, rel_dir))\n",
    "\n",
    "    # Process each split\n",
    "    for mode, img_dirs in splits.items():\n",
    "        data_rows = []\n",
    "        count = 0  # Track how many images have been added\n",
    "        for img_dir in img_dirs:\n",
    "            # Determine corresponding labels directory by replacing \"images\" with \"labels\"\n",
    "            label_dir = img_dir.replace(\"images\", \"labels\")\n",
    "            # List all PNG files in this directory\n",
    "            image_files = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(\".png\")])\n",
    "            for img_file in image_files:\n",
    "                if max_images is not None and count >= max_images:\n",
    "                    break  # Stop if limit reached\n",
    "                # Full path to the image\n",
    "                img_path = os.path.join(img_dir, img_file)\n",
    "                # Open image, convert to grayscale ('F')\n",
    "                image = Image.open(img_path).convert('F')\n",
    "                # Convert to numpy array and flatten (original size: 1920x1080)\n",
    "                img_array = np.array(image, dtype=np.float32).flatten()\n",
    "                \n",
    "                # Find the corresponding label file\n",
    "                label_filename = os.path.splitext(img_file)[0] + \".txt\"\n",
    "                label_path = os.path.join(label_dir, label_filename)\n",
    "                with open(label_path, \"r\") as lf:\n",
    "                    # For example: \"0 0.569076 0.381246 0.115152 0.130603\"\n",
    "                    label_line = lf.readline().strip()\n",
    "                    label_values = [float(x) for x in label_line.split()[1:]]\n",
    "                \n",
    "                # Concatenate flattened image and label values\n",
    "                row = np.concatenate([img_array, np.array(label_values, dtype=np.float32)])\n",
    "                data_rows.append(row)\n",
    "                count += 1\n",
    "            \n",
    "            if max_images is not None and count >= max_images:\n",
    "                break  # Don't process more folders if limit is reached\n",
    "        \n",
    "        # If any data is found, stack and save as .amat\n",
    "        if data_rows:\n",
    "            data_matrix = np.vstack(data_rows)\n",
    "            microrobot_type = config[\"names\"][0]  # e.g., \"cylinder\"\n",
    "            suffix = f\"_{max_images}\" if max_images is not None else \"\"\n",
    "            amat_filename = f\"USMicroMagSet_processed/ultrasound_{microrobot_type}_{mode}{suffix}.amat\"\n",
    "            np.savetxt(amat_filename, data_matrix, fmt=\"%.6f\")\n",
    "            print(f\"Saved {amat_filename} with shape {data_matrix.shape}\")\n",
    "        else:\n",
    "            print(f\"No images found for split {mode} in {microrobot_folder}.\")\n",
    "\n",
    "# Example usage (uncomment and set your folder path):\n",
    "max_images = 128\n",
    "preprocess_ultrasound_data(\"UsMicroMagSet-main/flagella\", max_images=max_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. The model\n",
    "\n",
    "Finally, we build a **Steerable CNN** and try it on USMicroMagSet.\n",
    "\n",
    "Let's also use a group a bit larger: we now build a model equivariant to $8$ rotations.\n",
    "We indicate the group of $N$ discrete rotations as $C_N$, i.e. the **cyclic group** of order $N$.\n",
    "In this case, we will use $C_8$.\n",
    "\n",
    "Because the inputs are still gray-scale images, the input type of the model is again a *scalar field*.\n",
    "\n",
    "However, internally we use *regular fields*: this is equivalent to a *group-equivariant convolutional neural network*.\n",
    "\n",
    "Finally, we build *invariant* features for the final classification task by pooling over the group using *Group Pooling*.\n",
    "\n",
    "The final classification is performed by a two fully connected layers.\n",
    "\n",
    "**Here is the definition of our model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C8SteerableCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, labels_points=4, input_size=256):\n",
    "        \n",
    "        super(C8SteerableCNN, self).__init__()\n",
    "        \n",
    "        # the model is equivariant under rotations by 45 degrees, modelled by C8\n",
    "        self.r2_act = gspaces.rot2dOnR2(N=8)\n",
    "        \n",
    "        # the input image is a scalar field, corresponding to the trivial representation\n",
    "        in_type = nn.FieldType(self.r2_act, [self.r2_act.trivial_repr])\n",
    "        \n",
    "        # we store the input type for wrapping the images into a geometric tensor during the forward pass\n",
    "        self.input_type = in_type\n",
    "        \n",
    "        # convolution 1\n",
    "        # first specify the output type of the convolutional layer\n",
    "        # we choose 24 feature fields, each transforming under the regular representation of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 24*[self.r2_act.regular_repr])\n",
    "        self.block1 = nn.SequentialModule(\n",
    "            nn.MaskModule(in_type, input_size, margin=1), # rotating a square image causes corners and edges to move into previously empty regions. This makes my image inherently not symmetric. Hence masking a circle around it to bring the symmetry back\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=7, padding=1, bias=False),\n",
    "            nn.InnerBatchNorm(out_type), # equivariant version of Batch Normalization. It normalizes all channels together\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution 2\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block1.out_type\n",
    "        # the output type of the second convolution layer are 48 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n",
    "        self.block2 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool1 = nn.SequentialModule(\n",
    "            nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        ) # pooling is invariant\n",
    "        \n",
    "        # convolution 3\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block2.out_type\n",
    "        # the output type of the third convolution layer are 48 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n",
    "        self.block3 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution 4\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block3.out_type\n",
    "        # the output type of the fourth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n",
    "        self.block4 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool2 = nn.SequentialModule(\n",
    "            nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        )\n",
    "        \n",
    "        # convolution 5\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block4.out_type\n",
    "        # the output type of the fifth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n",
    "        self.block5 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution 6\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block5.out_type\n",
    "        # the output type of the sixth convolution layer are 64 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 64*[self.r2_act.regular_repr])\n",
    "        self.block6 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False, stride=1),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool3 = nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=1, padding=0, )\n",
    "        \n",
    "        # self.gpool = nn.GroupPooling(out_type)\n",
    "        \n",
    "        # number of output channels\n",
    "        c = self.gpool.out_type.size\n",
    "        c = 207936 # WHY\n",
    "        print(c)\n",
    "\n",
    "        # TODO: Gives error when running model due to no space in mps?\n",
    "        # Detect flattened FC size\n",
    "        # with torch.no_grad():\n",
    "        #     dummy = torch.randn(1, 1, input_size, input_size)\n",
    "        #     dummy = nn.GeometricTensor(dummy, self.input_type)\n",
    "        #     x = self.block1(dummy)\n",
    "        #     x = self.block2(x); x = self.pool1(x)\n",
    "        #     x = self.block3(x); x = self.pool2(x)\n",
    "        #     x = self.block4(x); x = self.pool3(x)\n",
    "        #     x = self.gpool(x)\n",
    "        #     c = x.tensor.view(1, -1).shape[1]\n",
    "        \n",
    "        # Fully Connected\n",
    "        self.fully_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(c, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.ELU(inplace=True),\n",
    "            torch.nn.Linear(64, labels_points),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # wrap the input tensor in a GeometricTensor\n",
    "        # (associate it with the input type)\n",
    "        x = nn.GeometricTensor(input, self.input_type)\n",
    "\n",
    "        # apply each equivariant block\n",
    "\n",
    "        # Each layer has an input and an output type\n",
    "        # A layer takes a GeometricTensor in input.\n",
    "        # This tensor needs to be associated with the same representation of the layer's input type\n",
    "        #\n",
    "        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n",
    "        # As a result, consecutive layers need to have matching input/output types\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        \n",
    "        # pool over the spatial dimensions\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # pool over the group        \n",
    "        x = self.gpool(x)\n",
    "\n",
    "        # unwrap the output GeometricTensor\n",
    "        # (take the Pytorch tensor and discard the associated representation)\n",
    "        x = x.tensor\n",
    "        \n",
    "        # classify with the final fully connected layers)\n",
    "        print(x.shape)\n",
    "        x = self.fully_net(x.reshape(x.shape[0], -1))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(         \n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=3, # prob drop this to 3 (which is normally done)\n",
    "                stride=1,                   \n",
    "                padding=1, # not much of a benefit + computationally efficient w/out, but if it was 0, the dimensions would reduce to 26x26\n",
    "            ),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(         \n",
    "            torch.nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            torch.nn.ReLU(),                      \n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),                \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(32 * 7 * 7, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 10) # reduce number of nodes\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.fc(x)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "comp = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    comp = 'cuda' # nvidia gpu parallelization\n",
    "elif torch.backends.mps.is_available():\n",
    "    comp = 'mps' # mac Metal Performance Shaders (high performance gpu)\n",
    "\n",
    "device = torch.device(comp)\n",
    "device = torch.device('cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Dataset Class: USMicroMagDataset\n",
    "\n",
    "This class loads the preprocessed `.amat` file and makes the data available for your model. Each row in the file is expected to contain:\n",
    "\n",
    "- **Flattened image pixels** (from a 1920×1080 grayscale image).\n",
    "- **Label values** (for example, 5 numbers that include a class and bounding box coordinates).\n",
    "\n",
    "The class converts the image back into its 2D shape and applies any provided transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USMicroMagDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for ultrasound microrobot data using preprocessed .amat files.\n",
    "    \n",
    "    Each row in the .amat file is formatted as:\n",
    "      [ flattened_pixels (1920*1080 values)  label_values (e.g., 5 numbers) ]\n",
    "    \"\"\"\n",
    "    def __init__(self, mode, microrobot_type, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mode (str): 'train' or 'test'\n",
    "            microrobot_type (str): e.g., \"cylinder\"\n",
    "            transform (callable, optional): Transformations to apply to the image.\n",
    "        \"\"\"\n",
    "        assert mode.startswith('train') or mode.startswith('test'), \"Mode must start with 'train' or 'test'\" # can be smt like train_50\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Construct the filename from the microrobot type and mode\n",
    "        file = f\"USMicroMagSet_processed/ultrasound_{microrobot_type}_{mode}.amat\"\n",
    "        \n",
    "        # Load the .amat file (space-delimited floats)\n",
    "        data = np.loadtxt(file, delimiter=' ')\n",
    "        \n",
    "        # Define the original image dimensions\n",
    "        self.img_width = 1920\n",
    "        self.img_height = 1080\n",
    "        num_pixels = self.img_width * self.img_height\n",
    "        \n",
    "        # First num_pixels columns: image data; remaining: label values\n",
    "        self.images = data[:, :num_pixels].reshape(-1, self.img_height, self.img_width).astype(np.float32)\n",
    "        self.labels = data[:, num_pixels:]\n",
    "        self.num_samples = len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.images[index], self.labels[index]\n",
    "        # Convert numpy image to PIL Image (using mode 'F' for float images)\n",
    "        image = Image.fromarray(image, mode='F')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(label, dtype=torch.float32) # mps only works with float32 not 64\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "# Take a rectangular image and pad it into a square\n",
    "def pad_to_square(image, fill=0):\n",
    "    w, h = image.size\n",
    "    diff = abs(h - w)\n",
    "    pad1 = diff // 2\n",
    "    pad2 = diff - pad1                # in case its uneven\n",
    "    if w > h:\n",
    "        padding = (0, pad1, 0, pad2)  # top and bottom\n",
    "    else:\n",
    "        padding = (pad1, 0, pad2, 0)  # left and right\n",
    "    return Pad(padding, fill=fill)(image)\n",
    "\n",
    "resize1 = Resize(512)\n",
    "resize2 = Resize(256)\n",
    "totensor = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 Done\n",
      "Step 2 Done\n",
      "Step 3 Done\n",
      "Step 4 Done\n",
      "Step 5 Done\n"
     ]
    }
   ],
   "source": [
    "train_transform = Compose([\n",
    "    # pad_to_square, # resize from 1920x1080 to 1920x1920\n",
    "    # tried to upsample for rotation - resize to 2200\n",
    "    # kernel isn't loading, instead downsizing\n",
    "    # resize1,\n",
    "    # RandomRotation(180., interpolation=InterpolationMode.BILINEAR, expand=False), # data augmentation to help with generalization\n",
    "    resize2,\n",
    "    totensor\n",
    "])\n",
    "print(\"Step 1 Done\")\n",
    "test_transform = Compose([\n",
    "    # pad_to_square,\n",
    "    resize2,\n",
    "    totensor\n",
    "])\n",
    "\n",
    "print(\"Step 2 Done\")\n",
    "US_train = USMicroMagDataset(mode=f'train_{max_images}', microrobot_type='flagella', transform=train_transform)\n",
    "print(\"Step 3 Done\")\n",
    "train_loader = torch.utils.data.DataLoader(US_train, batch_size=64)\n",
    "print(\"Step 4 Done\")\n",
    "\n",
    "US_test = USMicroMagDataset(mode=f'test_{max_images}', microrobot_type='flagella', transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(US_test, batch_size=8)\n",
    "print(\"Step 5 Done\")\n",
    "\n",
    "loader = {\n",
    "    \"train\": train_loader,\n",
    "    \"test\": test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207936\n"
     ]
    }
   ],
   "source": [
    "model = C8SteerableCNN().to(device)\n",
    "\n",
    "# Can choose to load pretrained model here\n",
    "# model.load_state_dict(torch.load(f\"c8_steerable_bbox_model_SmoothL1Loss_{max_images}.pth\"))\n",
    "# model.to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15436388\n",
      "C8SteerableCNN(\n",
      "  (block1): SequentialModule(\n",
      "    (0): MaskModule()\n",
      "    (1): R2Conv([C8_on_R2[(None, 8)]: {irrep_0 (x1)}(1)], [C8_on_R2[(None, 8)]: {regular (x24)}(192)], kernel_size=7, stride=1, padding=1, bias=False)\n",
      "    (2): InnerBatchNorm([C8_on_R2[(None, 8)]: {regular (x24)}(192)], eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True, type=[C8_on_R2[(None, 8)]: {regular (x24)}(192)])\n",
      "  )\n",
      "  (block2): SequentialModule(\n",
      "    (0): R2Conv([C8_on_R2[(None, 8)]: {regular (x24)}(192)], [C8_on_R2[(None, 8)]: {regular (x48)}(384)], kernel_size=5, stride=1, padding=2, bias=False)\n",
      "    (1): InnerBatchNorm([C8_on_R2[(None, 8)]: {regular (x48)}(384)], eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True, type=[C8_on_R2[(None, 8)]: {regular (x48)}(384)])\n",
      "  )\n",
      "  (pool1): SequentialModule(\n",
      "    (0): PointwiseAvgPoolAntialiased2D()\n",
      "  )\n",
      "  (block3): SequentialModule(\n",
      "    (0): R2Conv([C8_on_R2[(None, 8)]: {regular (x48)}(384)], [C8_on_R2[(None, 8)]: {regular (x48)}(384)], kernel_size=5, stride=1, padding=2, bias=False)\n",
      "    (1): InnerBatchNorm([C8_on_R2[(None, 8)]: {regular (x48)}(384)], eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True, type=[C8_on_R2[(None, 8)]: {regular (x48)}(384)])\n",
      "  )\n",
      "  (block4): SequentialModule(\n",
      "    (0): R2Conv([C8_on_R2[(None, 8)]: {regular (x48)}(384)], [C8_on_R2[(None, 8)]: {regular (x96)}(768)], kernel_size=5, stride=1, padding=2, bias=False)\n",
      "    (1): InnerBatchNorm([C8_on_R2[(None, 8)]: {regular (x96)}(768)], eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True, type=[C8_on_R2[(None, 8)]: {regular (x96)}(768)])\n",
      "  )\n",
      "  (pool2): SequentialModule(\n",
      "    (0): PointwiseAvgPoolAntialiased2D()\n",
      "  )\n",
      "  (block5): SequentialModule(\n",
      "    (0): R2Conv([C8_on_R2[(None, 8)]: {regular (x96)}(768)], [C8_on_R2[(None, 8)]: {regular (x96)}(768)], kernel_size=5, stride=1, padding=2, bias=False)\n",
      "    (1): InnerBatchNorm([C8_on_R2[(None, 8)]: {regular (x96)}(768)], eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True, type=[C8_on_R2[(None, 8)]: {regular (x96)}(768)])\n",
      "  )\n",
      "  (block6): SequentialModule(\n",
      "    (0): R2Conv([C8_on_R2[(None, 8)]: {regular (x96)}(768)], [C8_on_R2[(None, 8)]: {regular (x64)}(512)], kernel_size=5, stride=1, padding=1, bias=False)\n",
      "    (1): InnerBatchNorm([C8_on_R2[(None, 8)]: {regular (x64)}(512)], eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True, type=[C8_on_R2[(None, 8)]: {regular (x64)}(512)])\n",
      "  )\n",
      "  (pool3): PointwiseAvgPoolAntialiased2D()\n",
      "  (gpool): GroupPooling([C8_on_R2[(None, 8)]: {regular (x64)}(512)])\n",
      "  (fully_net): Sequential(\n",
      "    (0): Linear(in_features=207936, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "    (3): Linear(in_features=64, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# print(summary(model, (2, 256,256))) # LOOK INTO THIS\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now randomly initialized. \n",
    "Therefore, we do not expect it to produce the right class probabilities.\n",
    "\n",
    "However, the model should still produce the same output for rotated versions of the same image.\n",
    "This is true for rotations by multiples of $\\frac{\\pi}{2}$, but is only approximate for rotations by $\\frac{\\pi}{4}$.\n",
    "\n",
    "Let's test it on a random test image:\n",
    "we feed eight rotated versions of the first image in the test set and print the output logits of the model for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: torch.nn.Module, x: Image):\n",
    "    np.set_printoptions(linewidth=10000)\n",
    "    \n",
    "    # evaluate the `model` on 8 rotated versions of the input image `x`\n",
    "    model.eval()\n",
    "    \n",
    "    x = resize1(pad_to_square(x))\n",
    "    \n",
    "    print()\n",
    "    print('##########################################################################################')\n",
    "    header = 'angle |  ' + '  '.join([\"{:6d}\".format(d) for d in range(10)])\n",
    "    print(header)\n",
    "    with torch.no_grad():\n",
    "        for r in range(8):\n",
    "            x_transformed = totensor(resize2(x.rotate(r*45., Image.BILINEAR))).reshape(1, 1, 256, 256)\n",
    "            x_transformed = x_transformed.to(device)\n",
    "\n",
    "            y = model(x_transformed)\n",
    "            y = y.to('cpu').numpy().squeeze()\n",
    "            \n",
    "            angle = r * 45\n",
    "            print(\"{:5d} : {}\".format(angle, y))\n",
    "    print('##########################################################################################')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the test set\n",
    "raw_US_test = USMicroMagDataset(mode=f'test_{max_images}', microrobot_type='flagella')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########################################################################################\n",
      "angle |       0       1       2       3       4       5       6       7       8       9\n",
      "torch.Size([1, 64, 57, 57])\n",
      "    0 : [  4.7724905 -36.43578   -35.465176  -20.464321 ]\n",
      "torch.Size([1, 64, 57, 57])\n",
      "   45 : [-18.895529   -1.2547625 -18.582905   -5.6299853]\n",
      "torch.Size([1, 64, 57, 57])\n",
      "   90 : [  5.7560062 -22.132225   -9.093501  -13.553826 ]\n",
      "torch.Size([1, 64, 57, 57])\n",
      "  135 : [ -8.4184475 -11.75095    -1.4405465  -4.766584 ]\n",
      "torch.Size([1, 64, 57, 57])\n",
      "  180 : [-24.746244 -28.30666   -7.950215  15.308436]\n",
      "torch.Size([1, 64, 57, 57])\n",
      "  225 : [ -9.521747     0.08192833  12.9338455  -17.617683  ]\n",
      "torch.Size([1, 64, 57, 57])\n",
      "  270 : [-3.264354  -0.9363069 16.38468   -4.3628917]\n",
      "torch.Size([1, 64, 57, 57])\n",
      "  315 : [ -4.6154814 -41.045734  -22.396006  -21.844591 ]\n",
      "##########################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrieve the first image from the test set\n",
    "x, y = next(iter(raw_US_test))\n",
    "\n",
    "# evaluate the model\n",
    "test_model(model, x)\n",
    "\n",
    "# This doesnt look like its invariant in the slightest vs the mnist one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's train the model now.\n",
    "The model is exactly the same used to train a normal *PyTorch* architecture:\n",
    "\n",
    "Note:\n",
    "- **MSE Loss (torch.nn.MSELoss)** penalizes the squared error between predictions and targets. It's sensitive to large errors (outliers) since errors are squared.\n",
    "\n",
    "- **Smooth L1 Loss (torch.nn.SmoothL1Loss)** is less sensitive to outliers because it behaves like an L1 loss for large errors and like an L2 loss for small errors. It’s often preferred in bounding box regression tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function = torch.nn.CrossEntropyLoss() # This is for classification tasks\n",
    "loss_function = torch.nn.MSELoss() # can use SmoothL1Loss() or MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the model\n",
    "\n",
    "Run `tensorboard --logdir=runs` in the terminal to see tensorboard logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Length of train loader 16\n",
      "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 6 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 7 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 8 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 9 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 10 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 11 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 12 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 13 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 14 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 15 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Epoch 0 | Test loss: 0.1485\n",
      "Epoch: 1\n",
      "Length of train loader 16\n",
      "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 6 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 7 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 8 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 9 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 10 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 11 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 12 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 13 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 14 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 15 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Epoch: 2\n",
      "Length of train loader 16\n",
      "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 6 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 7 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 8 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 9 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 10 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 11 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 12 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 13 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 14 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 15 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Epoch: 3\n",
      "Length of train loader 16\n",
      "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 6 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 7 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 8 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 9 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 10 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 11 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 12 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 13 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 14 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 15 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Epoch: 4\n",
      "Length of train loader 16\n",
      "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 6 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 7 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 8 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 9 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 10 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 11 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 12 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 13 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 14 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 15 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Epoch: 5\n",
      "Length of train loader 16\n",
      "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 6 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 7 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 8 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 9 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 10 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 11 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 12 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 13 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 14 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 15 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Epoch: 6\n",
      "Length of train loader 16\n",
      "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 6 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 7 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 8 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 9 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 10 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 11 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 12 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 13 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 14 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 15 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Epoch: 7\n",
      "Length of train loader 16\n",
      "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 6 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 7 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 8 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 9 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 10 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 11 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 12 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 13 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 14 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 15 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Epoch: 8\n",
      "Length of train loader 16\n",
      "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 6 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 7 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 8 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 9 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 10 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 11 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 12 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 13 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 14 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 15 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Epoch: 9\n",
      "Length of train loader 16\n",
      "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 6 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 7 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 8 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 9 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 10 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 11 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 12 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 13 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 14 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 15 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Epoch: 10\n",
      "Length of train loader 16\n",
      "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 6 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 7 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 8 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 9 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 10 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 11 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 12 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 13 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 14 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Batch 15 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
      "Epoch 10 | Test loss: 0.0988\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/c8_bbox_run\")  # customize the name if needed\n",
    "\n",
    "for epoch in range(11):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    model.train()\n",
    "    print(\"Length of train loader\", len(train_loader))\n",
    "    \n",
    "    epoch_train_loss = 0.0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        print(f\"Batch {i} | imgs.shape: {imgs.shape} | labels.shape: {labels.shape}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        y = model(imgs)\n",
    "        loss = loss_function(y, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate training loss\n",
    "        epoch_train_loss += loss.item() * imgs.size(0)\n",
    "        total_train_samples += imgs.size(0)\n",
    "\n",
    "    # Log average training loss for this epoch\n",
    "    avg_train_loss = epoch_train_loss / total_train_samples\n",
    "    writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n",
    "\n",
    "    # Evaluation step every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (imgs, labels) in enumerate(test_loader):\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device).float()\n",
    "                \n",
    "                y = model(imgs)\n",
    "                loss = loss_function(y, labels)\n",
    "                total_loss += loss.item() * imgs.size(0)\n",
    "                total_samples += imgs.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / total_samples\n",
    "        print(f\"Epoch {epoch} | Test loss: {avg_loss:.4f}\")\n",
    "        writer.add_scalar(\"Loss/Test\", avg_loss, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"pretrained_models/c8_steerable_bbox_model_MSELoss_{max_images}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Test and Visualize Model on Random Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 57, 57])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take one random image from the test set\n",
    "model.eval()\n",
    "\n",
    "# Get a single batch\n",
    "imgs, labels = next(iter(test_loader))\n",
    "index = random.randint(0, imgs.size(0) - 1)\n",
    "\n",
    "img = imgs[index].unsqueeze(0).to(device)  # shape: [1, 1, 256, 256]\n",
    "true_bbox = labels[index].cpu().numpy()    # shape: [4]\n",
    "\n",
    "# Predict bounding box\n",
    "with torch.no_grad():\n",
    "    pred_bbox = model(img).squeeze().cpu().numpy()\n",
    "\n",
    "# Rescale bbox to image size (256x256)\n",
    "def rescale_bbox(bbox, size=256):\n",
    "    cx, cy, w, h = bbox\n",
    "    x1 = (cx - w / 2) * size\n",
    "    y1 = (cy - h / 2) * size\n",
    "    x2 = (cx + w / 2) * size\n",
    "    y2 = (cy + h / 2) * size\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "pred_box_px = rescale_bbox(pred_bbox)\n",
    "true_box_px = rescale_bbox(true_bbox)\n",
    "\n",
    "# Plot the image with predicted and true bounding box\n",
    "img_np = imgs[index].squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img_np, cmap='gray')\n",
    "plt.title(\"Predicted (Red) vs Ground Truth (Green)\")\n",
    "\n",
    "# Draw predicted bbox\n",
    "plt.gca().add_patch(plt.Rectangle(\n",
    "    (pred_box_px[0], pred_box_px[1]),\n",
    "    pred_box_px[2] - pred_box_px[0],\n",
    "    pred_box_px[3] - pred_box_px[1],\n",
    "    linewidth=2, edgecolor='r', facecolor='none'\n",
    "))\n",
    "\n",
    "# Draw ground truth bbox\n",
    "plt.gca().add_patch(plt.Rectangle(\n",
    "    (true_box_px[0], true_box_px[1]),\n",
    "    true_box_px[2] - true_box_px[0],\n",
    "    true_box_px[3] - true_box_px[1],\n",
    "    linewidth=2, edgecolor='g', facecolor='none'\n",
    "))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy paste cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SmoothL1Loss\n",
    "\n",
    "Epoch: 0\n",
    "Length of train loader 7\n",
    "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 6 | imgs.shape: torch.Size([2, 1, 256, 256]) | labels.shape: torch.Size([2, 4])\n",
    "Epoch 0 | Test loss: 1.2231\n",
    "Epoch: 1\n",
    "Length of train loader 7\n",
    "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 6 | imgs.shape: torch.Size([2, 1, 256, 256]) | labels.shape: torch.Size([2, 4])\n",
    "Epoch: 2\n",
    "Length of train loader 7\n",
    "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 6 | imgs.shape: torch.Size([2, 1, 256, 256]) | labels.shape: torch.Size([2, 4])\n",
    "Epoch: 3\n",
    "Length of train loader 7\n",
    "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 6 | imgs.shape: torch.Size([2, 1, 256, 256]) | labels.shape: torch.Size([2, 4])\n",
    "Epoch: 4\n",
    "Length of train loader 7\n",
    "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 6 | imgs.shape: torch.Size([2, 1, 256, 256]) | labels.shape: torch.Size([2, 4])\n",
    "Epoch: 5\n",
    "Length of train loader 7\n",
    "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 6 | imgs.shape: torch.Size([2, 1, 256, 256]) | labels.shape: torch.Size([2, 4])\n",
    "Epoch: 6\n",
    "Length of train loader 7\n",
    "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 6 | imgs.shape: torch.Size([2, 1, 256, 256]) | labels.shape: torch.Size([2, 4])\n",
    "Epoch: 7\n",
    "Length of train loader 7\n",
    "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 6 | imgs.shape: torch.Size([2, 1, 256, 256]) | labels.shape: torch.Size([2, 4])\n",
    "Epoch: 8\n",
    "Length of train loader 7\n",
    "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 6 | imgs.shape: torch.Size([2, 1, 256, 256]) | labels.shape: torch.Size([2, 4])\n",
    "Epoch: 9\n",
    "Length of train loader 7\n",
    "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 6 | imgs.shape: torch.Size([2, 1, 256, 256]) | labels.shape: torch.Size([2, 4])\n",
    "Epoch: 10\n",
    "Length of train loader 7\n",
    "Batch 0 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 1 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 2 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 3 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 4 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 5 | imgs.shape: torch.Size([8, 1, 256, 256]) | labels.shape: torch.Size([8, 4])\n",
    "Batch 6 | imgs.shape: torch.Size([2, 1, 256, 256]) | labels.shape: torch.Size([2, 4])\n",
    "Epoch 10 | Test loss: 0.0504"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"desc_images/SmoothL1Loss.png\" alt=\"SmoothL1Loss Summary\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
